package routines;

import java.io.File;
import java.io.IOException;
import java.io.OutputStream;
import java.nio.channels.Channels;
import java.util.List;
import java.util.UUID;

import com.google.api.gax.paging.Page;
import com.google.auth.oauth2.GoogleCredentials;
import com.google.cloud.bigquery.BigQuery;
import com.google.cloud.bigquery.BigQueryException;
import com.google.cloud.bigquery.BigQueryOptions;
import com.google.cloud.bigquery.Dataset;
import com.google.cloud.bigquery.DatasetInfo;
import com.google.cloud.bigquery.Field;
import com.google.cloud.bigquery.FieldList;
import com.google.cloud.bigquery.FieldValueList;
import com.google.cloud.bigquery.FormatOptions;
import com.google.cloud.bigquery.Job;
import com.google.cloud.bigquery.JobId;
import com.google.cloud.bigquery.JobInfo;
import com.google.cloud.bigquery.JobInfo.CreateDisposition;
import com.google.cloud.bigquery.JobInfo.WriteDisposition;
import com.google.cloud.bigquery.LegacySQLTypeName;
import com.google.cloud.bigquery.LoadJobConfiguration;
import com.google.cloud.bigquery.QueryJobConfiguration;
import com.google.cloud.bigquery.Schema;
import com.google.cloud.bigquery.StandardTableDefinition;
import com.google.cloud.bigquery.TableDataWriteChannel;
import com.google.cloud.bigquery.TableId;
import com.google.cloud.bigquery.TableResult;
import com.google.cloud.bigquery.WriteChannelConfiguration;
import com.google.common.io.Files;
import com.google.gson.Gson;
import com.google.gson.GsonBuilder;
import com.google.gson.JsonDeserializer;


public class GoogleBigQuery {

	private static BigQuery bigquery(){
		GoogleCredentials credentials = GoogleAuth.getGoogleDefaultServiceAccountCredentials();

		return BigQueryOptions.newBuilder().setCredentials(credentials).build().getService();
	}

	public static void createDataset(String datasetName){
		DatasetInfo datasetInfo = DatasetInfo.newBuilder(datasetName)
				.setLocation("EU")
				.build();

		try {
			System.out.println("Creating dataset [" + datasetName + "]...");

			Dataset dataset = bigquery().create(datasetInfo);

			System.out.println("DatasetId [" + dataset.getDatasetId() + "] created");
		} catch(BigQueryException e) {
			//TODO dataset not created
			System.out.println(e.getReason() + ": " + e.getMessage());
		}
	}

	// Lists datasets in a specified project
	public static Page<Dataset> listDatasets(String projectId){
		Page<Dataset> datasets = bigquery().listDatasets(projectId);

		return datasets;
	}

	public static void query(String query, TableId destinationTable, WriteDisposition writeDisposition) 
			throws InterruptedException{
		QueryJobConfiguration queryConfig =
				QueryJobConfiguration.newBuilder(query)
				.setUseLegacySql(false)
				.setWriteDisposition(writeDisposition)
				.setDestinationTable(destinationTable)
				.setAllowLargeResults(true)
				.build();

		// Create a job ID so that we can safely retry.
		JobId jobId = JobId.of(UUID.randomUUID().toString());
		Job queryJob = bigquery().create(JobInfo.newBuilder(queryConfig)
				.setJobId(jobId)
				.build());

		// Wait for the query to complete.
		queryJob = queryJob.waitFor();

		// Check for errors
		if (queryJob == null) {
			throw new RuntimeException("Job no longer exists");
		} else if (queryJob.getStatus().getError() != null) {
			// You can also look at queryJob.getStatus().getExecutionErrors() for all
			// errors, not just the latest one.
			throw new RuntimeException(queryJob.getStatus().getError().toString());
		}

		System.out.println("JobId: " + queryJob.getJobId());
	}

	public static void query(String query) 
			throws InterruptedException{
		QueryJobConfiguration queryConfig =
				QueryJobConfiguration.newBuilder(query)
				.setUseLegacySql(false)
				.setAllowLargeResults(true)
				.build();

		// Create a job ID so that we can safely retry.
		JobId jobId = JobId.of(UUID.randomUUID().toString());
		Job queryJob = bigquery().create(JobInfo.newBuilder(queryConfig)
				.setJobId(jobId)
				.build());

		// Wait for the query to complete.
		queryJob = queryJob.waitFor();

		// Check for errors
		if (queryJob == null) {
			throw new RuntimeException("Job no longer exists");
		} else if (queryJob.getStatus().getError() != null) {
			// You can also look at queryJob.getStatus().getExecutionErrors() for all
			// errors, not just the latest one.
			throw new RuntimeException(queryJob.getStatus().getError().toString());
		}

		// Get the results.
		//QueryResponse response = bigquery.getQueryResults(jobId);
		//TableResult result = queryJob.getQueryResults();

		System.out.println("JobId: " + queryJob.getJobId());

		// Get the results.
		//QueryResponse response = bigquery.getQueryResults(jobId);
		TableResult result = queryJob.getQueryResults();

		// Print all pages of the results.
		for (FieldValueList row : result.iterateAll()) {
			String col = row.get("col").getStringValue();
			System.out.printf("col: %s", col);
		}
	}

	public static void loadTableFromGCS(String sourceUri, FormatOptions format, TableId tableId, WriteDisposition writeDisposition) {
		BigQuery bigquery = bigquery();

		LoadJobConfiguration configuration =
				LoadJobConfiguration.newBuilder(tableId, sourceUri)
				.setFormatOptions(format)
				.setAutodetect(true)
				.setWriteDisposition(writeDisposition)
				.build();

		// Load the table
		Job loadJob = bigquery.create(JobInfo.of(configuration));

		try {
			loadJob = loadJob.waitFor();

			if (loadJob != null && loadJob.getStatus().getError() == null) {
				// Job completed successfully
			} else {
				// Handle error case
				throw new RuntimeException(loadJob.getStatus().getExecutionErrors().toString());
			}
		} catch (InterruptedException e) {
			// Handle interrupted wait
			System.out.println(e.getMessage());
		}

		// Check the table
		System.out.println("JobId: " + loadJob.getJobId().getJob() + " - " + loadJob.getStatus().getState());
		System.out.println("Total number of rows: " +  ((StandardTableDefinition) bigquery.getTable(tableId).getDefinition()).getNumRows());
	}

	public static void loadTableFromGCS(String sourceUri, Schema schema, FormatOptions format, TableId tableId, WriteDisposition writeDisposition) {
		BigQuery bigquery = bigquery();

		LoadJobConfiguration configuration =
				LoadJobConfiguration.newBuilder(tableId, sourceUri)
				.setFormatOptions(format)
				.setSchema(schema)
				.setWriteDisposition(WriteDisposition.WRITE_TRUNCATE)
				.build();

		// Load the table
		Job loadJob = bigquery.create(JobInfo.of(configuration));

		try {
			loadJob = loadJob.waitFor();

			if (loadJob != null && loadJob.getStatus().getError() == null) {
				// Job completed successfully
			} else {
				// Handle error case
				throw new RuntimeException(loadJob.getStatus().getExecutionErrors().toString());
			}
		} catch (InterruptedException e) {
			// Handle interrupted wait
			System.out.println(e.getMessage());
		}

		// Check the table
		System.out.println("JobId: " + loadJob.getJobId().getJob() + " - " + loadJob.getStatus().getState());
		System.out.println("Total number of rows: " +  ((StandardTableDefinition) bigquery.getTable(tableId).getDefinition()).getNumRows());
	}

	public static void loadTableFromGCS(List<String> sourceUris, Schema schema, FormatOptions format, TableId tableId, WriteDisposition writeDisposition) {
		BigQuery bigquery = bigquery();

		LoadJobConfiguration configuration =
				LoadJobConfiguration.newBuilder(tableId, sourceUris)
				.setFormatOptions(format)
				.setCreateDisposition(CreateDisposition.CREATE_IF_NEEDED)
				.setSchema(schema)
				.setWriteDisposition(writeDisposition)
				.build();

		// Load the table
		Job loadJob = bigquery.create(JobInfo.of(configuration));

		try {
			loadJob = loadJob.waitFor();

			if (loadJob != null && loadJob.getStatus().getError() == null) {
				// Job completed successfully
			} else {
				// Handle error case
				throw new RuntimeException(loadJob.getStatus().getError().toString());
			}
		} catch (InterruptedException e) {
			// Handle interrupted wait
			System.out.println(e.getMessage());
		}

		// Check the table
		System.out.println("JobId: " + loadJob.getJobId().getJob() + " - " + loadJob.getStatus().getState());
		System.out.println("Total number of rows: " +  ((StandardTableDefinition) bigquery.getTable(tableId).getDefinition()).getNumRows());
	}

	public static void loadTableFromLocal(File path, FormatOptions format, TableId tableId, WriteDisposition writeDisposition){
		BigQuery bigquery = bigquery();

		WriteChannelConfiguration writeChannelConfiguration =
				WriteChannelConfiguration.newBuilder(tableId)
				.setCreateDisposition(CreateDisposition.CREATE_IF_NEEDED)
				.setFormatOptions(format)
				.setAutodetect(true)
				.setWriteDisposition(writeDisposition)
				.build();

		// The location must be specified; other fields can be auto-detected.
		JobId jobId = JobId.newBuilder().setLocation("EU").build();

		TableDataWriteChannel writer = bigquery.writer(jobId, writeChannelConfiguration);
		// Write data to writer
		try (OutputStream stream = Channels.newOutputStream(writer)) {
			Files.copy(path, stream);
		} catch (IOException e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
		}

		// Get load job
		Job loadJob = writer.getJob();
		try {
			loadJob = loadJob.waitFor();
			if (loadJob != null && loadJob.getStatus().getError() == null) {
				// Job completed successfully
			} else {
				// Handle error case
				throw new RuntimeException(loadJob.getStatus().getError().toString());
			}
		} catch (InterruptedException e) {
			// Handle interrupted wait
			System.out.println(e.getMessage());
		}

		// Check the table
		System.out.println("JobId: " + loadJob.getJobId().getJob() + " - " + loadJob.getStatus().getState());
		System.out.println("Total number of rows: " +  ((StandardTableDefinition) bigquery.getTable(tableId).getDefinition()).getNumRows());
	}

	public static void loadTableFromLocal (File path, Schema schema, FormatOptions format, TableId tableId, WriteDisposition writeDisposition){
		BigQuery bigquery = bigquery();

		WriteChannelConfiguration writeChannelConfiguration =
				WriteChannelConfiguration.newBuilder(tableId)
				.setCreateDisposition(CreateDisposition.CREATE_IF_NEEDED)
				.setFormatOptions(format)
				.setSchema(schema)
				.setWriteDisposition(writeDisposition)
				.build();

		// The location must be specified; other fields can be auto-detected.
		JobId jobId = JobId.newBuilder().setLocation("EU").build();

		TableDataWriteChannel writer = bigquery.writer(jobId, writeChannelConfiguration);
		// Write data to writer
		try (OutputStream stream = Channels.newOutputStream(writer)) {
			Files.copy(path, stream);
		} catch (IOException e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
		}

		// Get load job
		Job loadJob = writer.getJob();
		try {
			loadJob = loadJob.waitFor();
			if (loadJob != null && loadJob.getStatus().getError() == null) {
				// Job completed successfully
			} else {
				// Handle error case
				throw new RuntimeException(loadJob.getStatus().getError().toString());
			}
		} catch (InterruptedException e) {
			// Handle interrupted wait
			System.out.println(e.getMessage());
		}

		// Check the table
		System.out.println("JobId: " + loadJob.getJobId().getJob() + " - " + loadJob.getStatus().getState());
		System.out.println("Total number of rows: " +  ((StandardTableDefinition) bigquery.getTable(tableId).getDefinition()).getNumRows());
	}

	public static Gson getGson() {
		JsonDeserializer<LegacySQLTypeName> typeDeserializer = (jsonElement, type, deserializationContext) -> {
			return LegacySQLTypeName.valueOf(jsonElement.getAsString());
		};

		JsonDeserializer<FieldList> subFieldsDeserializer = (jsonElement, type, deserializationContext) -> {
			Field[] fields = deserializationContext.deserialize(jsonElement.getAsJsonArray(), Field[].class);
			return FieldList.of(fields);
		};

		return new GsonBuilder()
		.registerTypeAdapter(LegacySQLTypeName.class, typeDeserializer)
		.registerTypeAdapter(FieldList.class, subFieldsDeserializer)
		.create();
	}

	public static void test(){
		//BigQuery bigquery = bigquery();

	}
}